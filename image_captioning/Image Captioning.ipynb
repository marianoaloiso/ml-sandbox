{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show and Tell\n",
    "\"Show and Tell\" is basic model architecture where a Convolutional Neural Network (CNN) is used to extract image features, which are then fed into a Long Short-Term Memory (LSTM) network to generate captions.\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "```Python\n",
    "import torch \n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "class ShowAndTellModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ShowAndTellModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Encoder (CNN)\n",
    "        self.encoder = models.resnet50()\n",
    "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Decoder (RNN)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        # Encode images\n",
    "        features = self.encoder(images)\n",
    "        features = self.dropout(self.relu(features))\n",
    "\n",
    "        # Decode captions\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "\n",
    "        return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained models\n",
    "cnn = models.resnet50()\n",
    "lstm = nn.LSTM(input_size=2048, hidden_size=512, num_layers=1)  # Example LSTM\n",
    "\n",
    "# Remove the last layer of the CNN\n",
    "modules = list(cnn.children())[:-1]\n",
    "cnn = nn.Sequential(*modules)\n",
    "\n",
    "# Define the image transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowAndTellModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ShowAndTellModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Encoder (CNN)\n",
    "        self.encoder = models.resnet50()\n",
    "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Decoder (RNN)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.decoder = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.linear = nn.Linear(self.decoder.config.n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encode images\n",
    "        features = self.encoder(images)\n",
    "        features = self.dropout(self.relu(features))\n",
    "\n",
    "        # Decode captions\n",
    "        embeddings = self.decoder(features)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "\n",
    "        # Convert embeddings to tokens\n",
    "        tokens = [self.tokenizer.encode(sentence, add_special_tokens=True) for sentence in embeddings]\n",
    "        tokens_tensor = torch.tensor(tokens)\n",
    "\n",
    "        outputs = self.decoder(input_ids=tokens_tensor)\n",
    "        outputs = self.linear(outputs.last_hidden_state)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "image = Image.open(\"images/heinz.jpg\")\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "embed_size = 512  # Example size, you can adjust based on your requirements\n",
    "hidden_size = 512  # Example size, you can adjust based on your requirements\n",
    "vocab_size = 10000  # Example size, you should set it based on your vocabulary size\n",
    "num_layers = 1  # Example number of layers, you can adjust based on your requirements\n",
    "\n",
    "# Initialize the model instance\n",
    "model = ShowAndTellModel(embed_size, hidden_size, vocab_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m input_image \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add an extra dimension at the beginning for batch size\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Pass the image through the ShowAndTellModel\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming your model instance is named model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m output_captions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 27\u001b[0m, in \u001b[0;36mShowAndTellModel.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     24\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(features))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Decode captions\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((features\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), embeddings), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Convert embeddings to tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:837\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-sandbox/.venv/lib/python3.9/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to match the input size of ResNet-50\n",
    "    transforms.ToTensor(),           # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Apply the preprocessing steps to the image\n",
    "input_image = preprocess(image).unsqueeze(0)  # Add an extra dimension at the beginning for batch size\n",
    "\n",
    "# Pass the image through the ShowAndTellModel\n",
    "# Assuming your model instance is named model\n",
    "output_captions = model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the image using the CNN\n",
    "with torch.no_grad():\n",
    "    features = cnn(input_batch)\n",
    "\n",
    "# Prepare the features for the LSTM\n",
    "features = features.view(1, -1)\n",
    "\n",
    "# Generate the caption using the LSTM\n",
    "hidden = (torch.randn(1, 1, 512), torch.randn(1, 1, 512))  # Example hidden state\n",
    "output, _ = lstm(features.unsqueeze(0), hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3498e-01, -1.3846e-01, -3.5764e-02,  2.0683e-01,  3.8889e-01,\n",
       "           2.8958e-01, -6.5076e-01,  1.2353e-02,  4.4398e-01, -1.9044e-01,\n",
       "          -1.8730e-02, -1.1456e-01, -7.6114e-02, -2.1864e-02,  1.7474e-01,\n",
       "           6.1594e-01, -5.7552e-03,  3.4448e-01,  2.8342e-01, -2.2675e-01,\n",
       "          -6.4897e-02, -1.8656e-01, -1.3069e-01,  6.4673e-01,  1.8054e-01,\n",
       "          -8.3798e-02, -1.7044e-01,  1.2797e-01,  3.7055e-01,  1.1754e-02,\n",
       "          -4.2841e-01,  1.8044e-01,  1.7171e-01, -4.6599e-02,  5.5438e-01,\n",
       "          -3.4377e-02, -1.3975e-01, -3.2244e-01, -1.7203e-01, -7.3425e-03,\n",
       "           1.9933e-01, -3.7366e-01,  1.6867e-01,  1.7807e-01,  2.1114e-01,\n",
       "          -5.4326e-01,  5.0138e-02, -2.0958e-02, -1.8931e-01,  6.5047e-02,\n",
       "           6.4941e-02, -1.0451e-01,  3.1256e-01,  2.3992e-01, -4.2102e-02,\n",
       "          -9.0220e-02,  1.4510e-01,  7.8737e-02,  3.2548e-01, -3.0161e-02,\n",
       "          -4.6526e-01,  1.1463e-01, -6.8010e-02, -3.1349e-02,  4.6978e-02,\n",
       "           5.2373e-03, -4.4084e-01, -7.5780e-02, -4.5629e-01,  3.0848e-01,\n",
       "           5.9042e-01, -1.2284e-01,  7.6613e-02,  2.9204e-01, -2.6564e-01,\n",
       "          -6.7916e-02, -1.0800e-01, -3.8747e-02, -1.3948e-01,  4.4063e-01,\n",
       "          -8.6909e-02,  8.1585e-02,  5.7364e-01,  1.9421e-02, -2.0261e-01,\n",
       "          -1.7847e-01,  2.2376e-02,  3.4996e-01, -3.0554e-01,  4.9581e-02,\n",
       "           6.6613e-02, -1.3361e-01, -8.4086e-03, -3.1772e-02,  3.1388e-01,\n",
       "          -5.4954e-02, -1.9096e-02,  7.3876e-02, -4.4285e-01,  5.2480e-01,\n",
       "          -3.0885e-02,  1.6354e-01, -3.6978e-01,  3.7303e-01, -3.0960e-02,\n",
       "          -1.9909e-01, -3.2463e-01,  3.1472e-01, -8.9555e-02,  3.8390e-02,\n",
       "           1.9013e-01, -1.0853e-01,  2.0365e-01,  2.5679e-01, -3.7480e-01,\n",
       "          -4.5596e-02,  4.5483e-02, -2.2290e-01, -4.5052e-01, -2.4103e-02,\n",
       "           6.8518e-01, -1.4749e-02,  2.2647e-01,  6.6722e-03,  1.2911e-01,\n",
       "           3.4497e-01,  4.0172e-01, -3.3057e-01,  1.7257e-01,  2.6369e-03,\n",
       "           3.0038e-01, -2.0908e-01, -2.8112e-01,  8.0659e-03,  5.9605e-02,\n",
       "          -5.7520e-01,  5.9888e-02,  7.6081e-02, -2.0418e-01, -1.6660e-01,\n",
       "          -7.8707e-01, -5.8223e-01,  1.4335e-01, -4.6002e-04,  3.9553e-03,\n",
       "          -4.5162e-01,  1.3030e-01, -5.3382e-01,  1.0259e-01, -6.0970e-02,\n",
       "           4.4332e-02, -3.7272e-01,  8.3543e-02,  3.1547e-01, -8.4321e-03,\n",
       "          -2.9048e-02,  3.1757e-01, -3.4098e-01, -1.0804e-01,  3.4356e-01,\n",
       "          -1.7465e-02, -2.8666e-01,  1.2338e-01, -1.6955e-01,  1.6548e-01,\n",
       "          -4.2963e-01,  1.9772e-01,  4.0483e-01,  1.0479e-01, -1.8865e-01,\n",
       "           5.4138e-01, -1.7849e-01, -1.0133e-01,  1.6520e-01, -4.2605e-01,\n",
       "           3.0078e-01,  5.0802e-01,  4.0835e-01,  8.3299e-03, -1.9094e-01,\n",
       "           1.4302e-01, -4.1218e-02,  6.0867e-01,  3.2403e-01, -2.0842e-01,\n",
       "           8.5966e-02,  6.0772e-03, -4.9511e-01, -2.6275e-02,  2.8970e-02,\n",
       "           2.2628e-02,  2.6460e-01,  2.1393e-01,  6.5145e-01,  2.4194e-01,\n",
       "          -9.2268e-02, -4.9036e-01, -3.0960e-02, -5.3750e-01, -5.3766e-02,\n",
       "           5.1074e-02,  7.1683e-02,  5.4505e-03,  1.5797e-01,  4.3893e-02,\n",
       "           3.7261e-02, -1.4490e-01,  1.9419e-01, -4.3821e-02, -4.1590e-01,\n",
       "           1.1996e-01, -3.7264e-01, -8.3331e-02,  3.7632e-02,  5.8729e-01,\n",
       "           5.2388e-02,  1.7385e-01,  2.1370e-01, -4.3753e-01,  2.3785e-01,\n",
       "           1.4903e-01,  7.6769e-02,  3.1667e-01, -2.5223e-01, -7.6463e-02,\n",
       "          -1.2574e-01,  2.7742e-01,  4.8465e-02, -1.8980e-01, -1.0661e-01,\n",
       "          -1.4616e-02,  1.2518e-01,  5.1196e-01,  9.4929e-02, -2.0492e-01,\n",
       "           6.4473e-02,  1.3596e-01,  1.1821e-01, -2.6840e-02, -3.8129e-01,\n",
       "           6.6840e-01,  1.4954e-01,  5.4616e-02, -1.8612e-01, -2.6550e-01,\n",
       "           1.5844e-01,  2.8445e-01,  1.2806e-01,  3.3521e-01, -3.1439e-01,\n",
       "           7.7061e-01,  2.3478e-01,  6.0878e-01, -2.7789e-02, -2.0803e-01,\n",
       "          -4.0932e-02, -3.9663e-01,  4.0762e-01, -4.8310e-02,  4.1744e-02,\n",
       "           2.9627e-01,  4.8231e-02,  7.1360e-01, -4.4124e-01, -2.4118e-01,\n",
       "          -2.9645e-01,  4.5744e-02,  5.1045e-02,  1.2649e-01,  4.2724e-02,\n",
       "           3.1110e-01,  9.4903e-02, -6.8967e-02, -1.4758e-01, -1.5046e-02,\n",
       "           3.5153e-01,  2.2986e-01,  1.0679e-01, -3.0097e-01,  1.9839e-01,\n",
       "          -2.1613e-01,  6.6576e-02,  6.4817e-04, -1.0060e-01, -8.8440e-04,\n",
       "          -9.6311e-02, -1.8252e-01,  7.2533e-02, -1.5857e-02,  4.4580e-01,\n",
       "           9.4536e-03, -2.4725e-01,  6.8046e-01,  1.3977e-01, -5.4407e-01,\n",
       "          -1.1989e-01, -1.8736e-02,  2.6126e-01, -1.8423e-01, -1.9775e-02,\n",
       "          -2.6398e-01,  1.4831e-01,  3.9585e-03, -6.8074e-02, -1.3392e-01,\n",
       "           2.1066e-01, -2.8526e-01,  5.1549e-01,  2.7905e-01,  4.4044e-01,\n",
       "           1.3257e-01,  5.2430e-02,  6.7751e-02,  1.9691e-01, -1.8388e-01,\n",
       "           1.3780e-01,  3.1483e-02, -4.8460e-01, -1.5623e-01,  3.9035e-01,\n",
       "           5.5863e-01, -1.1589e-02, -1.3952e-02,  5.5172e-01,  4.7464e-01,\n",
       "           2.7776e-01, -4.8553e-01, -2.3858e-01,  6.1526e-01, -2.8295e-01,\n",
       "          -8.5309e-03, -1.7968e-01,  6.9268e-01,  1.2080e-01,  3.1496e-01,\n",
       "           2.1842e-01,  1.4597e-02, -1.1497e-01, -1.2969e-02,  8.5279e-03,\n",
       "           1.3069e-01, -2.7330e-01,  5.7325e-02, -2.2050e-01,  6.5197e-02,\n",
       "          -3.7459e-02, -3.8156e-01,  4.4987e-02,  7.7965e-02,  3.6266e-02,\n",
       "           9.1890e-02,  3.4056e-03,  5.7390e-01, -1.9631e-01, -4.2080e-01,\n",
       "          -7.3235e-02, -2.0301e-02, -1.2299e-01,  2.7692e-01, -8.2281e-02,\n",
       "           2.6593e-01, -5.7722e-03, -4.5549e-01, -1.4878e-01,  1.2364e-01,\n",
       "           3.4397e-01, -1.0951e-01,  2.3928e-01,  1.9365e-01,  1.2331e-01,\n",
       "          -1.9447e-01,  2.2684e-02,  3.1476e-01,  4.5990e-01, -6.4292e-01,\n",
       "          -3.2408e-01, -5.1537e-01, -1.9084e-02, -2.8093e-01, -4.2353e-02,\n",
       "           4.7014e-03, -2.6885e-01, -5.3813e-02, -9.9787e-02,  5.9633e-01,\n",
       "          -8.7149e-03,  8.9686e-02,  2.5303e-01, -2.7600e-01,  3.8021e-01,\n",
       "          -2.2791e-01,  3.5209e-01, -5.7964e-01, -1.2587e-01, -4.7510e-01,\n",
       "           3.0159e-01,  4.8244e-01, -1.6071e-01, -9.4944e-02, -2.4036e-01,\n",
       "          -1.6911e-01,  2.7705e-01, -1.2873e-01,  6.6681e-02,  1.4187e-01,\n",
       "           2.1289e-02, -2.1015e-01,  7.8599e-02, -8.7943e-03, -1.7265e-01,\n",
       "          -1.8768e-01,  2.6847e-01, -1.0235e-01,  1.1246e-01,  8.5544e-02,\n",
       "          -8.0229e-01,  5.0483e-02,  2.9096e-01, -2.6957e-01,  3.7365e-02,\n",
       "           1.6881e-01,  8.1444e-02, -1.3128e-01,  2.2414e-01,  9.6236e-02,\n",
       "          -7.6724e-02,  2.1905e-01, -6.0177e-02, -1.6390e-01, -1.6764e-01,\n",
       "          -2.8219e-01,  4.4524e-01, -9.1161e-02,  5.7058e-01, -3.0298e-01,\n",
       "          -1.9635e-01,  5.3525e-01,  3.9462e-01, -9.4803e-02, -1.1570e-01,\n",
       "           6.9759e-02, -9.6164e-03, -6.0780e-02,  2.5865e-01,  2.1440e-01,\n",
       "          -4.8894e-01,  1.1614e-01, -9.4958e-02, -4.0105e-01,  3.5371e-02,\n",
       "           7.3081e-01,  2.9046e-01, -3.4014e-02,  3.3915e-02,  1.4195e-03,\n",
       "           8.5091e-02, -2.6608e-01, -6.9025e-01, -1.5104e-01, -5.4718e-02,\n",
       "          -8.0116e-02, -5.6611e-02, -7.9896e-02,  2.3238e-02, -6.9458e-01,\n",
       "          -1.1795e-01,  1.3201e-01, -1.1889e-01,  3.8172e-01,  1.0649e-01,\n",
       "          -3.7842e-01, -1.5413e-01,  1.0945e-01,  7.6802e-02,  5.5982e-02,\n",
       "           3.9542e-03, -3.3464e-01, -4.3457e-01,  6.1172e-02,  1.0613e-01,\n",
       "          -7.8390e-02,  1.2376e-01,  1.4081e-02,  2.8016e-01, -1.8583e-01,\n",
       "          -6.1277e-02, -8.1304e-02, -7.8288e-02, -1.1296e-01, -4.0700e-02,\n",
       "           5.5635e-01, -3.5623e-01, -1.8871e-01, -1.0800e-01,  1.2926e-01,\n",
       "          -6.0514e-02, -4.5127e-02, -2.7519e-01, -9.2396e-02, -5.9081e-02,\n",
       "          -6.8005e-01,  3.4347e-01, -1.6414e-01, -2.7216e-01,  5.7236e-01,\n",
       "           2.5771e-01, -2.0764e-01, -1.3315e-01,  9.9558e-02,  1.8001e-01,\n",
       "           4.2118e-01,  2.6370e-02]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisualBERT\n",
    "\n",
    "Transformer-based model for image captioning tasks. It utilizes the attention mechanism and self-attention layers to capture long-range dependencies in both images and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
